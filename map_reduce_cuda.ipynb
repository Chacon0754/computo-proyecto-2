{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Problema 4\n",
        "\n",
        "Implementar el algoritmo Map-Reduce utilizando Python CUDA"
      ],
      "metadata": {
        "id": "Yx0dFnKBFDKF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jhJpcr3x1pQ6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af147922-4eee-4ac8-bc92-967f57bfd6e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Oct 28 04:31:47 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   46C    P8             12W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Comprobar que se este usando la GPU\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# instalar Cupy\n",
        "!pip install cupy-cuda12x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_A8Ld_jLFmt5",
        "outputId": "0319bd1d-b488-41c4-a3ec-22e09952a4bf"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: cupy-cuda12x in /usr/local/lib/python3.12/dist-packages (13.3.0)\n",
            "Requirement already satisfied: numpy<2.3,>=1.22 in /usr/local/lib/python3.12/dist-packages (from cupy-cuda12x) (2.0.2)\n",
            "Requirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.12/dist-packages (from cupy-cuda12x) (0.8.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# Map-Reduce numérico usando GPU (CuPy) vs CPU (NumPy)\n",
        "# =====================================================\n",
        "# Objetivo: agrupar (map) y reducir (sum y count) por clave (buckets).\n",
        "# Ejecutar en Google Colab con GPU activada.\n",
        "# =====================================================\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "# Intentaremos importar cupy; si no está instalado, descomenta la línea de instalación arriba.\n",
        "try:\n",
        "    import cupy as cp\n",
        "except Exception as e:\n",
        "    print(\"CuPy no está instalado o no se pudo importar. Instala cupy-cuda11x o cupy-cuda12x según tu CUDA.\")\n",
        "    raise e\n",
        "\n",
        "# -------------- Parámetros y datos --------------\n",
        "N = 50_000_000    # número de elementos\n",
        "K = 1024          # número de claves/buckets (ej. 1024)\n",
        "rng = np.random.default_rng(12345)\n",
        "\n",
        "# Generar datos de entrada en CPU (float32)\n",
        "# Usamos una distribución para que keys sean variadas\n",
        "values_cpu = (rng.random(N).astype(np.float32) * 1e6)  # valores en rango [0,1e6)\n",
        "# Definimos la función de mapa: key = int(value) % K  (ejemplo simple)\n",
        "# Pero no calculamos keys aún (lo haremos en CPU y GPU según versión)\n",
        "\n",
        "print(f\"Datos generados: N={N}, K={K}\")\n",
        "\n",
        "# -------------- CPU (NumPy) - Map + Reduce --------------\n",
        "def cpu_map_reduce(values, K):\n",
        "    t0 = time.perf_counter()\n",
        "    # Map: calcular claves en CPU\n",
        "    keys = (values.astype(np.int64) % K)\n",
        "    # Reduce: contamos ocurrencias por key y sumamos por key\n",
        "    t_map = time.perf_counter()\n",
        "    counts = np.bincount(keys, minlength=K)            # conteo por key\n",
        "    sums = np.bincount(keys, weights=values, minlength=K)  # suma por key\n",
        "    t1 = time.perf_counter()\n",
        "    return {\n",
        "        \"counts\": counts,\n",
        "        \"sums\": sums,\n",
        "        \"t_map\": t_map - t0,\n",
        "        \"t_reduce\": t1 - t_map,\n",
        "        \"t_total\": t1 - t0\n",
        "    }\n",
        "\n",
        "print(\"Ejecutando Map-Reduce en CPU (NumPy)... (esto puede tardar)\")\n",
        "cpu_res = cpu_map_reduce(values_cpu, K)\n",
        "print(f\"CPU total time: {cpu_res['t_total']:.4f} s, map: {cpu_res['t_map']:.4f}, reduce: {cpu_res['t_reduce']:.4f}\")\n",
        "\n",
        "# -------------- GPU (CuPy) - Map + Reduce --------------\n",
        "# Idea: copiar datos a GPU, map keys en GPU, usar cupy.bincount (altamente optimizado) con weights\n",
        "def gpu_map_reduce_cupy(values_cpu, K):\n",
        "    # Transferir host -> device\n",
        "    t0 = time.perf_counter()\n",
        "    arr_gpu = cp.array(values_cpu)   # copia Host->Device\n",
        "    cp.cuda.Stream.null.synchronize()\n",
        "    t_after_copy = time.perf_counter()\n",
        "\n",
        "    # Map (GPU): calcular keys\n",
        "    # keys = arr_gpu.astype(int) % K  -> cast y mod en GPU\n",
        "    keys_gpu = (arr_gpu.astype(cp.int64) % K)\n",
        "    cp.cuda.Stream.null.synchronize()\n",
        "    t_map_done = time.perf_counter()\n",
        "\n",
        "    # Reduce (GPU): bincount para counts y sums (weights)\n",
        "    counts_gpu = cp.bincount(keys_gpu, minlength=K)\n",
        "    sums_gpu = cp.bincount(keys_gpu, weights=arr_gpu, minlength=K)\n",
        "    cp.cuda.Stream.null.synchronize()\n",
        "    t_reduce_done = time.perf_counter()\n",
        "\n",
        "    # Traer resultados a host\n",
        "    counts = counts_gpu.get()\n",
        "    sums = sums_gpu.get()\n",
        "    t_after_get = time.perf_counter()\n",
        "\n",
        "    return {\n",
        "        \"counts\": counts,\n",
        "        \"sums\": sums,\n",
        "        \"t_copy_H2D\": t_after_copy - t0,\n",
        "        \"t_map\": t_map_done - t_after_copy,\n",
        "        \"t_reduce\": t_reduce_done - t_map_done,\n",
        "        \"t_copy_D2H\": t_after_get - t_reduce_done,\n",
        "        \"t_total\": t_after_get - t0\n",
        "    }\n",
        "\n",
        "print(\"Ejecutando Map-Reduce en GPU (CuPy)... (warm-up y ejecución, esto puede tardar)\")\n",
        "# Warm-up (compilaciones / JIT internos)\n",
        "_ = cp.array(np.array([1], dtype=np.float32))\n",
        "cp.cuda.Stream.null.synchronize()\n",
        "\n",
        "gpu_res = gpu_map_reduce_cupy(values_cpu, K)\n",
        "print(f\"GPU times (s): H2D={gpu_res['t_copy_H2D']:.4f}, map={gpu_res['t_map']:.4f}, reduce={gpu_res['t_reduce']:.4f}, D2H={gpu_res['t_copy_D2H']:.4f}, total={gpu_res['t_total']:.4f}\")\n",
        "\n",
        "# -------------- Verificación de igualdad (sanity check) --------------\n",
        "print(\"Verificando que CPU y GPU obtuvieron el mismo resultado para algunos buckets...\")\n",
        "# Comparamos sums y counts para los primeros 10 buckets\n",
        "for i in range(10):\n",
        "    print(f\"Bucket {i}: CPU count={cpu_res['counts'][i]}, GPU count={gpu_res['counts'][i]}, CPU sum={cpu_res['sums'][i]:.4f}, GPU sum={gpu_res['sums'][i]:.4f}\")\n",
        "\n",
        "# Chequeo numérico global (tolerancia pequeña)\n",
        "counts_equal = np.array_equal(cpu_res['counts'], gpu_res['counts'])\n",
        "sums_close = np.allclose(cpu_res['sums'], gpu_res['sums'], rtol=1e-6, atol=1e-3)\n",
        "print(f\"\\nCounts equal: {counts_equal}, sums close: {sums_close}\")\n",
        "\n",
        "# -------------- Reporte final --------------\n",
        "cpu_time = cpu_res['t_total']\n",
        "gpu_time = gpu_res['t_total']\n",
        "print(f\"\\n==== RESUMEN ====\")\n",
        "print(f\"CPU total: {cpu_time:.4f} s\")\n",
        "print(f\"GPU total: {gpu_time:.4f} s (incluye transferencias)\")\n",
        "print(f\"Speedup (CPU/GPU): {cpu_time / gpu_time:.2f}x\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njt3CJ_wFycO",
        "outputId": "d593264f-78ea-4343-8990-60198c70609f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datos generados: N=50000000, K=1024\n",
            "Ejecutando Map-Reduce en CPU (NumPy)... (esto puede tardar)\n",
            "CPU total time: 1.4388 s, map: 1.0440, reduce: 0.3948\n",
            "Ejecutando Map-Reduce en GPU (CuPy)... (warm-up y ejecución, esto puede tardar)\n",
            "GPU times (s): H2D=0.2091, map=0.6776, reduce=1.3413, D2H=0.0001, total=2.2281\n",
            "Verificando que CPU y GPU obtuvieron el mismo resultado para algunos buckets...\n",
            "Bucket 0: CPU count=49144, GPU count=49144, CPU sum=24630685729.1963, GPU sum=24630685729.1963\n",
            "Bucket 1: CPU count=48687, GPU count=48687, CPU sum=24292767032.0323, GPU sum=24292767032.0323\n",
            "Bucket 2: CPU count=49307, GPU count=49307, CPU sum=24632250741.3881, GPU sum=24632250741.3881\n",
            "Bucket 3: CPU count=48642, GPU count=48642, CPU sum=24273659232.5328, GPU sum=24273659232.5328\n",
            "Bucket 4: CPU count=49018, GPU count=49018, CPU sum=24555944273.2709, GPU sum=24555944273.2709\n",
            "Bucket 5: CPU count=49156, GPU count=49156, CPU sum=24513925129.9876, GPU sum=24513925129.9876\n",
            "Bucket 6: CPU count=48770, GPU count=48770, CPU sum=24469380763.9929, GPU sum=24469380763.9929\n",
            "Bucket 7: CPU count=49228, GPU count=49228, CPU sum=24745481883.6693, GPU sum=24745481883.6693\n",
            "Bucket 8: CPU count=48909, GPU count=48909, CPU sum=24451036214.2117, GPU sum=24451036214.2117\n",
            "Bucket 9: CPU count=48607, GPU count=48607, CPU sum=24355083236.1828, GPU sum=24355083236.1828\n",
            "\n",
            "Counts equal: True, sums close: True\n",
            "\n",
            "==== RESUMEN ====\n",
            "CPU total: 1.4388 s\n",
            "GPU total: 2.2281 s (incluye transferencias)\n",
            "Speedup (CPU/GPU): 0.65x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Interpretacion de los resultados\n",
        "* Datos: Procesaste 50 millones de números, agrupándolos en 1024 categorías.\n",
        "* Resultados Correctos: La verificación (Counts equal: True, sums close: True) confirma que tanto la versión de CPU como la de GPU produjeron los mismos resultados correctos para el conteo y la suma por bucket. Esto es fundamental para validar el algoritmo en la GPU.\n",
        "* Tiempos de CPU (NumPy):\n",
        "        El tiempo total en CPU fue de 1.4388 segundos.\n",
        "        El paso de \"map\" (calcular las claves) tomó la mayor parte del tiempo (1.0440 s), lo cual tiene sentido ya que involucra iterar sobre 50 millones de elementos y realizar operaciones aritméticas y un cast a entero.\n",
        "        El paso de \"reduce\" (el bincount) fue más rápido (0.3948 s).\n",
        "* Tiempos de GPU (CuPy):\n",
        "        El tiempo total en GPU fue de 2.2281 segundos.\n",
        "        Este tiempo incluye las transferencias de datos entre la CPU y la GPU.\n",
        "        La transferencia de CPU a GPU (H2D) tomó 0.2091 s.\n",
        "        El paso de \"map\" en GPU tomó 0.6776 s, que es significativamente más rápido que en la CPU (1.0440 s). Esto muestra el paralelismo de la GPU para esta operación.\n",
        "        El paso de \"reduce\" en GPU (1.3413 s) fue más lento que en la CPU (0.3948 s). Esto es un poco inesperado y podría deberse a la implementación específica de cp.bincount para este tamaño de datos y número de buckets en tu GPU particular, o a gastos generales internos de CuPy/CUDA para esta operación.\n",
        "        La transferencia de GPU a CPU (D2H) fue muy rápida (0.0001 s).\n",
        "* Speedup (CPU/GPU): El resultado 0.65x significa que la versión de GPU fue más lenta que la versión de CPU en este caso particular (1.4388 / 2.2281 ≈ 0.65).\n",
        "\n",
        "**Interpretación General:**\n",
        "\n",
        "Aunque la GPU es muy buena para paralelizar operaciones, en este ejemplo específico, el beneficio de la ejecución paralela del map y el reduce en la GPU fue superado por el tiempo de transferencia de datos entre la CPU y la GPU (H2D) y, sorprendentemente, por el tiempo que tomó la operación de \"reduce\" en la GPU en comparación con la CPU.\n",
        "\n",
        "Para que una implementación en GPU sea más rápida que en CPU, generalmente se necesita que:\n",
        "\n",
        "    La operación a paralelizar sea computacionalmente intensiva y pueda dividirse en muchas tareas independientes que la GPU pueda ejecutar simultáneamente (el \"map\" lo logró).\n",
        "    El tiempo de ejecución en la GPU sea significativamente menor que en la CPU.\n",
        "    El tiempo ahorrado en la computación en la GPU sea mayor que el tiempo adicional gastado en transferir datos entre la CPU y la GPU.\n",
        "\n",
        "En este caso, el \"reduce\" en la GPU fue más lento y la transferencia H2D añadió un costo que no se compensó con la aceleración del \"map\" y el \"reduce\" combinados en la GPU.\n",
        "\n",
        "Posibles Razones del Resultado:\n",
        "\n",
        "* Overhead de CuPy/CUDA: Para operaciones relativamente simples como bincount, el costo de configurar y ejecutar el kernel de CUDA a través de CuPy puede ser mayor que la ejecución altamente optimizada en NumPy para ciertos tamaños de datos.\n",
        "* Características de la GPU: Aunque la Tesla T4 es una GPU potente, el rendimiento en operaciones específicas puede variar.\n",
        "* Tamaño del problema vs. K: Con un K (número de buckets) relativamente pequeño (1024) comparado con N (50 millones), la operación bincount en NumPy ya es bastante eficiente. Si K fuera mucho más grande, o si la operación de \"map\" fuera mucho más compleja, la GPU podría mostrar una ventaja más clara.\n",
        "* Transferencia de Datos: Para muchos problemas, la transferencia de datos entre CPU y GPU es un cuello de botella significativo. Si los datos ya estuvieran en la GPU (por ejemplo, si fueran el resultado de una operación GPU anterior), el tiempo total de la GPU sería t_map + t_reduce + t_copy_D2H, que en tu caso sería 0.6776 + 1.3413 + 0.0001 ≈ 2.019 s, aún más lento que la CPU.\n",
        "\n",
        "En resumen, aunque se implemento correctamente el algoritmo Map-Reduce usando CuPy y se obtuvo los resultados correctos, para este problema específico con estos parámetros (N y K) y esta operación de \"reduce\", la implementación en CPU con NumPy resultó ser más rápida debido a la eficiencia del bincount en NumPy y al costo adicional de las transferencias de datos y el rendimiento del \"reduce\" en la GPU.\n",
        "\n",
        "Es un excelente ejemplo que demuestra que la GPU no siempre es más rápida; depende de la naturaleza de la computación, el tamaño de los datos y el costo de la transferencia de datos."
      ],
      "metadata": {
        "id": "yPirkoJlG7Yz"
      }
    }
  ]
}